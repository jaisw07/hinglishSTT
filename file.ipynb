{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e03b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50359, 50360, 50361, 50362, 50363], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, mera naam shri jaisvaal hai aur yah audio main record kar raha hoon to warm up the whisper STT app which we are trying to demo and give it an input of both hindi and english languages taaki yah sahi se initialize ho jaata hai and our warm up text, our warm up audio sorry, contains both languages. So, this is it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ffmpeg\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU) and precision\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Specify the pre-trained model ID\n",
    "model_id = \"Oriserve/Whisper-Hindi2Hinglish-Apex\"\n",
    "\n",
    "# Load the speech-to-text model with specified configurations\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,        # Use appropriate precision (float16 for GPU, float32 for CPU)\n",
    "    low_cpu_mem_usage=True,         # Optimize memory usage during loading\n",
    "    use_safetensors=True            # Use safetensors format for better security\n",
    ")\n",
    "model.to(device)                    # Move model to specified device\n",
    "\n",
    "# Load the processor for audio preprocessing and tokenization\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create speech recognition pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\n",
    "        \"task\": \"transcribe\",       # Set task to transcription\n",
    "        \"language\": \"en\"            # Specify English language\n",
    "    }\n",
    ")\n",
    "\n",
    "# Process audio file and print transcription\n",
    "sample = r\"C:\\Users\\SHREY\\Desktop\\hinglish_stt\\audio2.mp3\"               # Input audio file path\n",
    "result = pipe(sample)               # Run inference\n",
    "print(result[\"text\"])               # Print transcribed text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
